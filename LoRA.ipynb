{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA (Low Rank Adaption)\n",
    "\n",
    "LoRA allowing you to finetune only a small number of extra weights in the model while freezing most of the parameters of the pre trained network.\n",
    "\n",
    "Key idea here: we are not actually training the original weights. We are adding some extra weights. We are going to fine tune those.\n",
    "\n",
    "One of the main advantages of LoRA is: We have got original weights so this tends to help with stopping catastrophic forgetting. (Catastrophic forgetting: Models tend to forget what they were originally trained on when you do a fine tuning. If you do fine tuning too much, you may end up with catastrophic forgetting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA not only makes fine tuning way faster but it also generates these small adapters that you can plag and play together with your model to get your model solved specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why LoRA?\n",
    "\n",
    "A foundation model knows how to do many things, but it is not great at many tasks. We can fine-tune the model to produce specialized models that are very good at solving specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is not creating copies of foundation model. (copies that are specialized on different tasks) We get this from regular fine tuning process.\n",
    "\n",
    "In LoRA, we are talking about adapters whihc are small neural networks that they can plug in into different layers of the foundation models.\n",
    "\n",
    "In summary, there exist a foundation model plus small adapters that are going to make that foundation model act differently.\n",
    "\n",
    "We can load these adapters together with a model to dynamically transform its capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The logic of adapters\n",
    "Something that I can dynamically load, do an addition and get a completely new model that acts differently.\n",
    "\n",
    "When loading the model, we will take the foundation model's original weights and apply the LoRA weight changes to it to get the fine tuned model weights. \n",
    "\n",
    "Foundation model's original weights + LoRA weight changes = Specialized model's fine tuned weights\n",
    "\n",
    "![Lora Beginning](images/lorabegining.png)\n",
    "\n",
    "However there is a problem above. We want that adapters should be small. As we can see the adapter has the same size with the foundation model. What is going on?\n",
    "\n",
    "The beauty of LoRA is that we don't need to fine tune the entire matrix of weights. Instead, we can get away by fine-tuning two matrices of lower rank. These matrices, when multiplied together, will get us the weight updates we will need to apply the foundation model to modify its capabilities. The main logic is: We can represent a big matrix as multiplication of two matrices that are smaller.\n",
    "\n",
    "![Lora](images/lora.png)\n",
    "\n",
    "\n",
    "The adapter is not the LoRA weight changes in the first photo. The adapter is these 2 smaller matrices.\n",
    "\n",
    "\n",
    "At run time, I can load my foundation model, load the adapter, do the multiplication(fast operation). After that we get the weight changes. Get those weight changes and add them up to the original weights to get the fine tune weights. \n",
    "Here is how much you can save when using LoRA to fine tune models of different sizes:\n",
    "\n",
    "![GainLora](images/gainLoRA.png)\n",
    "\n",
    "Of course, this is just an illustiration. LoRA is not just 2 matrices. Ä°t is 2 matrices for each one of the layers where we want to apply LoRA.\n",
    "\n",
    "Savings are huge so in terms of storing this model or these adapters we are going to have huge decrease in memory and disk space.\n",
    "\n",
    "\n",
    "Also, realize that instead of trying to fine tune every single parameter individually we can focus on finding or fine tuning these 2 matrices here. Basically take 2 matrices and find values that multiplied give us list of changes that added up to the original model get us closer to our objective. (In that way fine tuning process is getting huge boost since we are only changing these 2 matrices' parameters.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers accelerate evaluate datasets peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21843 classes) at resolution 224x224 and fine tuned on ImageNet 2012 (1 million images, 1000 classes) at resolution 224x224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a size of 346 MB on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will fine tune a vision transformer\n",
    "# first aim: improve it (recognizing the items)\n",
    "# second aim: good at recognizing the differences (cat vs dog)\n",
    "model_checkpoint = \"google/vit-base-patch16-224-in21k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a couple of helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "\n",
    "# this gives the size of the model in that way we can measure the saving\n",
    "def print_model_size(path):\n",
    "    size = 0\n",
    "    for f in os.scandir(path):\n",
    "        size += os.path.getsize(f)\n",
    "    print(f\"Model size: {(size / 1e6):.2} MB\")\n",
    "\n",
    "\n",
    "# this shows how many parameters we are going to train in that way we can measure the savings on parameters\n",
    "def print_trainable_parameters(model, label):\n",
    "    parameters, trainable = 0, 0\n",
    "\n",
    "    for _, p in model.named_parameters():\n",
    "        parameters += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "\n",
    "    print(f\"{label} trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")\n",
    "\n",
    "\n",
    "def split_dataset(dataset):\n",
    "    dataset_splits = dataset.train_test_split(test_size = 0.1)\n",
    "    return dataset_splits.values()\n",
    "\n",
    "\n",
    "\n",
    "def create_label_mappings(dataset):\n",
    "    # label2id means i.e. orange is label and 3 is id of orange.It keeps as orange:3\n",
    "    # id2label means i.e. 3 is id orange is label of id 3. It keeps as 3:orange\n",
    "    label2id, id2label = dict(), dict()\n",
    "    for i, label in enumerate(dataset.features[\"label\"].names):\n",
    "        label2id[label] = i\n",
    "        id2label[i] = label\n",
    "\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Dataset\n",
    "\n",
    "I will be loading two different dataset to fine-tune the base model:\n",
    "\n",
    "1. A dataset of pictures of food.\n",
    "2. A dataset of pictures of cats and dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This is food dataset\n",
    "dataset1 = load_dataset(\"microsoft/cats_vs_dogs\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "dataset1_train, dataset1_test = split_dataset(dataset1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need these mappings to properly fine tune the Vision Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset1.rename_column(\"labels\", \"label\")\n",
    "\n",
    "dataset1_label2id, dataset1_id2label = create_label_mappings(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary is going to help me basically train or fine tune both models by just using one section for model1 and one section for model2.\n",
    "# the below means: when I am going to fine tune model1 I will use dataset1_train as train data and it will iterate 5 epochs etc.\n",
    "# the number of epochs affects the fine tune process \n",
    "config = {\n",
    "    \"model1\": {\n",
    "        \"train_data\": dataset1_train,\n",
    "        \"test_data\": dataset1_test,\n",
    "        \"label2id\": dataset1_label2id,\n",
    "        \"id2label\": dataset1_id2label,\n",
    "        \"epochs\": 1,\n",
    "        \"path\": \"./lora-model1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an image processor automatically from the processor configuration specified by the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint, use_fast = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now prepare the processing pipeline to transform the images in my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "preprocess_pipeline = Compose([\n",
    "    # resize the image\n",
    "    Resize(image_processor.size[\"height\"]),\n",
    "    # centeralize the image\n",
    "    CenterCrop(image_processor.size[\"height\"]),\n",
    "    # turn mage to tensor\n",
    "    ToTensor(),\n",
    "    # normalizing the image using mean and standart deviation\n",
    "    Normalize(mean=image_processor.image_mean, std = image_processor.image_std),\n",
    "])\n",
    "\n",
    "def preprocess(batch):\n",
    "    # for every image in batch converting each image to RGB, take it through the pipeline and add it to a batch with pixel values\n",
    "    batch[\"pixel_values\"] = [\n",
    "        preprocess_pipeline(image.convert(\"RGB\")) for image in batch[\"image\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "# Let's see the transform function to every train and test sets\n",
    "for cfg in config.values():\n",
    "    # grab the train data and specify the transformation processn for train data it will be preprocess function\n",
    "    cfg[\"train_data\"].set_transform(preprocess)\n",
    "    cfg[\"test_data\"].set_transform(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "The below functions will help us to fine tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def data_collate(examples):\n",
    "    \"\"\"\n",
    "    Prepare a batch of examples frokm a list of elements of the train or test datasets.\n",
    "    \"\"\"\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute the model's accuracy on a batch of predictions. I am using accuracy too understand whether my fine tuning process is working or not\n",
    "    \"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis = 1)\n",
    "    return metric.compute(predictions=predictions, references= eval_pred.label_ids)\n",
    "\n",
    "\n",
    "def get_base_model(label2id, id2label):\n",
    "    \"\"\"\n",
    "    Create an image classification base model from the model checkpoint.\n",
    "    \"\"\"\n",
    "    # This basically load the model from model_chcekpoint the original Vision Transformer model.\n",
    "    return AutoModelForImageClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        label2id = label2id,\n",
    "        id2label = id2label,\n",
    "        #ignore_mismatched_size = True,\n",
    "    )\n",
    "\n",
    "\n",
    "# This function builds the LoRA model\n",
    "def build_lora_model(label2id, id2label):\n",
    "    \"\"\"Build the LoRA model to fine tune the base model\"\"\"\n",
    "\n",
    "    model = get_base_model(label2id, id2label)\n",
    "    print_trainable_parameters(model, label = \"Base model\")\n",
    "\n",
    "    # this specifies how I want to do LoRA\n",
    "    config = LoraConfig(\n",
    "        r=16, #rank(represent adapters) to be more accurate increase the rank. Accuracy is not changing too much when the rank changes between 8 and 256\n",
    "        lora_alpha=16,\n",
    "        target_modules= [\"query\", \"value\"],\n",
    "        lora_dropout= 0.1,\n",
    "        bias = \"none\",\n",
    "        modules_to_save= [\"classifier\"],\n",
    "    )\n",
    "\n",
    "    # this is going to give us the actual model that we are going to fine tune\n",
    "    lora_model = get_peft_model(model, config)\n",
    "    print_trainable_parameters(lora_model, label=\"LoRA\")\n",
    "\n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 128\n",
    "training_arguments = TrainingArguments(\n",
    "    # parameters of training process (in this case it is just for fine tuning)\n",
    "    output_dir=\"./model-checkpoints\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    # I used float point 16 (I need gpu here)\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    # I want to compare the accuracy changes before and after fine tuning so I used accuracy\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "for cfg in config.values():\n",
    "    training_arguments.num_train_epochs = cfg[\"epochs\"]\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        # first parameter of Trainer is which model are we going to be fine tuning (this will be the result of the build_lora_model function)\n",
    "        build_lora_model(cfg[\"label2id\"], cfg[\"id2label\"]),\n",
    "        training_arguments,\n",
    "        # train data use for fine tuning\n",
    "        train_dataset=cfg[\"train_data\"],\n",
    "        # dataset to evaluate the fine tuning process\n",
    "        eval_dataset=cfg[\"test_data\"],\n",
    "        tokenizer=image_processor,\n",
    "        # how to compute metrics\n",
    "        compute_metrics=compute_metrics,\n",
    "        # how to prepare data to take it through model\n",
    "        data_collator=data_collate,\n",
    "    )\n",
    "\n",
    "    # train model (this is going to do the fine tuning process)\n",
    "    results = trainer.train()\n",
    "    # evaluate the model\n",
    "    evaluation_results = trainer.evaluate(cfg['test_data'])\n",
    "    print(f\"Evaluation accuracy: {evaluation_results['eval_accuracy']}\")\n",
    "\n",
    "    # We can now save the fine-tuned model to disk. (the saving here is just saving for LoRA adapter not the huge model)\n",
    "    trainer.save_model(cfg[\"path\"])\n",
    "    print_model_size(cfg[\"path\"])\n",
    "\n",
    "\n",
    "    # As we can observe below, in regular fine tuning we will have to train/ fine tune 85 million parameters. Instead, we are going to do LoRA. LoRA trainable parameters are only 591 thousand. (Only 0.68% of the original model. That's why the LoRA is faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of Fine Tune\n",
    "\n",
    "![LoraResult](images/loraoutput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have small adapters and big model. I don't have to load the original model again and again when I need to change the adapter on that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the mappings that I need to configure the classification heads and the adapter path (path where the adapter is)\n",
    "# return an already modified or fine tune model that specialize on solving one specific task (I did not have to modify the original big model. Original big model stays unchanged)\n",
    "def build_inference_model(label2id, id2label, lora_adapter_path):\n",
    "    \"\"\"Build the model that will be use to run inference.\"\"\"\n",
    "\n",
    "    # Let's load the base model\n",
    "    model = get_base_model(label2id, id2label)\n",
    "\n",
    "    # Now, we can create the inference model combining the base model\n",
    "    # with the fine-tuned LoRA adapter.\n",
    "    return PeftModel.from_pretrained(model, lora_adapter_path)\n",
    "\n",
    "\n",
    "# pass image_processor since I have to preprocess tha image before I send it to the model\n",
    "def predict(image, model, image_processor):\n",
    "    \"\"\"Predict the class represented by the supplied image.\"\"\"\n",
    "    \n",
    "    # process the image\n",
    "    encoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        # compute the outputs and get logits\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # find the most important item on those logits\n",
    "    class_index = logits.argmax(-1).item()\n",
    "    return model.config.id2label[class_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cfg in config.values():\n",
    "    # inference model is that each model is going to use\n",
    "    cfg[\"inference_model\"] = build_inference_model(cfg[\"label2id\"], cfg[\"id2label\"], cfg[\"path\"]) \n",
    "    # image processor is that each model should use\n",
    "    cfg[\"image_processor\"] = AutoImageProcessor.from_pretrained(cfg[\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    # I am specifying that this is image and this is the model that I want to use to classify this image   \n",
    "    {\n",
    "        # chicken wings image\n",
    "        \"image\": \"https://www.allrecipes.com/thmb/AtViolcfVtInHgq_mRtv4tPZASQ=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/ALR-187822-baked-chicken-wings-4x3-5c7b4624c8554f3da5aabb7d3a91a209.jpg\",\n",
    "        \"model\": \"model1\",\n",
    "    },\n",
    "    {\n",
    "        # pizza image\n",
    "        \"image\": \"https://www.simplyrecipes.com/thmb/KE6iMblr3R2Db6oE8HdyVsFSj2A=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/__opt__aboutcom__coeus__resources__content_migration__simply_recipes__uploads__2019__09__easy-pepperoni-pizza-lead-3-1024x682-583b275444104ef189d693a64df625da.jpg\",\n",
    "        \"model\": \"model1\"\n",
    "    }\n",
    "]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# for loop to go through all of the samples\n",
    "for sample in samples:\n",
    "    # open the image url\n",
    "    image = Image.open(requests.get(sample[\"image\"], stream=True).raw)\n",
    "    \n",
    "    # grab the inference model and image processor from the specific model specifying these samples\n",
    "    inference_model = config[sample[\"model\"]][\"inference_model\"]\n",
    "    image_processor = config[sample[\"model\"]][\"image_processor\"]\n",
    "\n",
    "    # take prediction from given this image, inference model and image processor\n",
    "    prediction = predict(image, inference_model, image_processor)\n",
    "    print(f\"Prediction: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
